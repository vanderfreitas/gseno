{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romul\\AppData\\Local\\Temp\\ipykernel_10564\\2769827032.py:10: RuntimeWarning: Could not add vertex ids, there is already an 'id' vertex attribute. at src/io/graphml.c:492\n",
      "  graph = Graph.Read_GraphML(\"Datas/networks/grafo_Peso_Geral.GraphML\")\n"
     ]
    }
   ],
   "source": [
    "from igraph import Graph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from matplotlib import rc\n",
    "# Open the GraphML file and create a Graph object from it\n",
    "graph = Graph.Read_GraphML(\"Datas/networks/grafo_Peso_Geral.GraphML\")\n",
    "min_cases = [1]\n",
    "boolQ= True\n",
    "# Initialize lists to store average similarity values for each column\n",
    "degrees_integral = []\n",
    "betweenness_integral = []\n",
    "clustering_integral = []\n",
    "strength_integral = []\n",
    "closeness_w_integral = []\n",
    "eignv_w_integral = []\n",
    "upper_random_avg = []\n",
    "lower_random_avg = []\n",
    "degrees_avg = []\n",
    "betweenness_avg = []\n",
    "clustering_avg = []\n",
    "strength_avg = []\n",
    "closeness_w_avg = []\n",
    "eignv_w_avg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used to transform vertex data from 'graph' into a matrix\n",
    "def get_matrix():\n",
    "    # Get vertex information\n",
    "    # Calculate the inverse of edge weights and store them in the 'w_inv' property\n",
    "    graph.es['w_inv'] = 1.0 / np.array(graph.es['weight'])\n",
    "    vertex_info = []\n",
    "    geocodes = graph.vs[\"geocode\"]\n",
    "    degrees = graph.degree()\n",
    "    betweenness = graph.betweenness(vertices=None, directed=False, cutoff=None, weights='w_inv')\n",
    "    clustering = graph.transitivity_local_undirected()\n",
    "    strength = graph.strength(weights=\"weight\")\n",
    "    closeness_w = graph.closeness(vertices=None, mode='all', cutoff=None, weights='w_inv', normalized=True)\n",
    "    eignv_w = graph.evcent(directed=False, scale=True, weights='w_inv', return_eigenvalue=False)\n",
    "    #prank_w = nx.pagerank(g_nx, alpha=0.85, weight='weight')\n",
    "    #vuln_w = vn.vulnerability(graph, weights='w_inv')\n",
    "    geocodes_int = list(map(int, geocodes))\n",
    "    geocodes_strings = list(map(str,geocodes_int))\n",
    "    # Construct the vertex information matrix\n",
    "    vertex_info = list(zip(geocodes_strings, degrees, betweenness, clustering, strength, closeness_w, eignv_w))\n",
    "\n",
    "    return vertex_info\n",
    "\n",
    "# Filter cases from a CSV file based on a minimum number of cases\n",
    "# and return a list of filtered elements and their data\n",
    "def filter_cases(csv_file, n):\n",
    "    # Read the CSV file and define the columns to be considered\n",
    "    df = pd.read_csv(\n",
    "        csv_file,\n",
    "        encoding='utf-8',\n",
    "        sep=',',\n",
    "        usecols=['ibgeID', 'newCases', 'totalCases', 'date'],\n",
    "        dtype={'ibgeID': int}  # Define the appropriate data type for ibgeID if possible\n",
    "    )\n",
    "    # Filter records that meet the requirements\n",
    "    filtered_df = df[(df['totalCases'] >= n) & (df['newCases'] >= 1) & (df['ibgeID'] != 0) & (df['ibgeID'] > 1000)]\n",
    "    # Remove duplicate records based on ibgeID\n",
    "    filtered_df = filtered_df.drop_duplicates(subset='ibgeID')\n",
    "    # Return the results as a list of tuples\n",
    "    filtered_cases = list(zip(filtered_df['date'], filtered_df['ibgeID'].apply(repr)))\n",
    "    return filtered_cases\n",
    "\n",
    "def setUp_comparsion_table(cases):\n",
    "# Set up the comparison matrix\n",
    "    comparison_matrix = np.array([Id_matrix_covid, degrees_similarity, betweenness_similarity, clustering_similarity,\n",
    "                              strength_similarity, closeness_w_similarity, eignv_w_similarity])\n",
    "    Table_Names = [\"DATES\", \"degrees\", \"betweenness\", \"clustering\", \"strength\", \"closeness_w\", \"eignv_w\"]\n",
    "    Final_Table = pd.DataFrame(comparison_matrix.T, columns=Table_Names)\n",
    "# Save the final table to an Excel file\n",
    "    excel_name = f\"/content/drive/MyDrive/Colab Notebooks/IC/result_Compa_table-{cases}cases.xlsx\"\n",
    "    Final_Table.to_excel(excel_name, index=False)\n",
    "\n",
    "# Filter records from list A based on a filtered list of cities with more than N Covid cases\n",
    "def filter_records(list_A, list_B):\n",
    "    # Create sets of geocodes for easy verification\n",
    "    geocode_set_A = set(record[0] for record in list_A)\n",
    "    geocode_set_B = set(record[1] for record in list_B)\n",
    "    # Find geocodes that exist in both sets\n",
    "    common_geocodes = geocode_set_A.intersection(geocode_set_B)\n",
    "    # Filter records that meet the criteria\n",
    "    filtered_list = [record for record in list_A if record[0] in common_geocodes]\n",
    "    return filtered_list\n",
    "\n",
    "def filter_records2(list_A, list_B):\n",
    "    # Create a set of all geocodes in list A for easy verification\n",
    "    geocode_set_A = set(record[0] for record in list_A)\n",
    "    # Filter records in list B that meet the criteria\n",
    "    filtered_list = [record for record in list_B if record[1] in geocode_set_A]\n",
    "    return filtered_list\n",
    "# Verify the similarity between an ordered list of cities based on a certain metric\n",
    "# and the list of cities with B Covid cases over time\n",
    "def compare_random(metrics_matrix):\n",
    "    # Select the columns from the matrices\n",
    "    result = []\n",
    "    for i in range(1, matrix_size + 1):\n",
    "        col1_elements = set(Id_matrix_covid[:i])\n",
    "        col2_elements = set(metrics_matrix[:i, 0].astype(float))\n",
    "        intersection = col1_elements.intersection(col2_elements)\n",
    "        similarity_percentage = (len(intersection) / len(col1_elements))\n",
    "        result.append(similarity_percentage)\n",
    "    return result\n",
    "\n",
    "def compare_columns(metrics_matrix, col_idx):\n",
    "    # Select the columns from the matrices\n",
    "    result = []\n",
    "    aux = metrics_matrix[metrics_matrix[:, col_idx].argsort()[::-1]]\n",
    "\n",
    "    for i in range(1, matrix_size + 1):\n",
    "        col1_elements = set(Id_matrix_covid[:i])\n",
    "\n",
    "        col2_elements = set(aux[:i, 0].astype(float))\n",
    "        intersection = col1_elements.intersection(col2_elements)\n",
    "        similarity_percentage = (len(intersection) / len(col1_elements))\n",
    "\n",
    "        result.append(similarity_percentage)\n",
    "    return result\n",
    "\n",
    "def calculate_random_metric_averages(data_matrix):\n",
    "\n",
    "    Mselected_data = data_matrix.copy()\n",
    "    # Extraia a segunda coluna (coluna 'geocode')\n",
    "    # Embaralhe a lista de geocodes\n",
    "    for _ in range(10):\n",
    "        np.random.shuffle(Mselected_data)\n",
    "    # Atualize a segunda coluna com os geocodes embaralhados\n",
    "    similarity = compare_random(Mselected_data)\n",
    "    result_data = {\n",
    "        \"Similarity\": similarity}\n",
    "    result_date_df = pd.DataFrame(result_data)\n",
    "    result_date_df['id'] = range(1, len(result_date_df) + 1)\n",
    "    return result_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_plot_DateXMetrics(step,cases,leng):\n",
    "    mean_table = combined_table.groupby('id').mean()\n",
    "    std_table = combined_table.groupby('id').std()\n",
    "    upper_bound = mean_table + 2 * std_table\n",
    "    lower_bound = mean_table - 2 * std_table\n",
    "    dados={\n",
    "        \"upper_random_avg\": upper_bound['Similarity'],\n",
    "        \"lower_random_avg\": lower_bound['Similarity']\n",
    "    }\n",
    "    df =pd.DataFrame(dados)\n",
    "    print(df)\n",
    "    df.to_csv('dados_similaridade.csv',index=\"False\")\n",
    "\n",
    "    # Indices for points along the x-axis intervals\n",
    "    # point_indices = np.linspace(0, len(matrix_covid_dates)-1, num=100)\n",
    "    # date_indices = np.arange(len(matrix_covid_dates))\n",
    "    # # Interpolation of similarity points\n",
    "    # degrees_interp = np.interp(point_indices, date_indices, degrees_similarity)\n",
    "    # betweenness_interp = np.interp(point_indices, date_indices, betweenness_similarity)\n",
    "    # clustering_interp = np.interp(point_indices, date_indices, clustering_similarity)\n",
    "    # strength_interp = np.interp(point_indices, date_indices, strength_similarity)\n",
    "    # closeness_w_interp = np.interp(point_indices, date_indices, closeness_w_similarity)\n",
    "    # eignv_w_interp = np.interp(point_indices, date_indices, eignv_w_similarity)\n",
    "    # # Store the interpolation data in a pandas DataFrame\n",
    "    # interp_data_df = pd.DataFrame({\n",
    "    # 'Degree': degrees_interp,\n",
    "    # 'Weighted Betweenness': betweenness_interp,\n",
    "    # 'Clustering': clustering_interp,\n",
    "    # 'Weighted Strength': strength_interp,\n",
    "    # 'Weighted Closeness': closeness_w_interp,\n",
    "    # 'Weighted Eigenvector': eignv_w_interp })\n",
    "\n",
    "    # # Configure the plot\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.plot(point_indices, degrees_interp, 'ro-', label='Degree', marker='s')\n",
    "    # plt.plot(point_indices, betweenness_interp, 'go-', label='Weighted Betweenness', marker='^')\n",
    "    # plt.plot(point_indices, clustering_interp, 'yo-', label='Clustering', marker='v')\n",
    "    # plt.plot(point_indices, strength_interp, 'mo-', label='Weighted Strength', marker='x')\n",
    "    # plt.plot(point_indices, closeness_w_interp, 'co-', label=' Weighted Closeness ', marker='+')\n",
    "    # plt.plot(point_indices, eignv_w_interp, 'ko-', label='Weighted Eigenvector ',marker='o')\n",
    "    # plt.fill_between(mean_table.index, lower_bound['Similarity'], upper_bound['Similarity'], color='gray', alpha=0.3)\n",
    "    # plt.fill_between([], [], [], color='gray', alpha=0.3, label='Development with Random Cases')\n",
    "    # plt.text(0.5, 0.93, f'Number of Cities: {leng}', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "  \n",
    "    # plt.legend()\n",
    "    \n",
    "    # # Adjust the display of the x-axis\n",
    "    # visible_indices = np.arange(0, len(matrix_covid_dates), step=step)\n",
    "    # visible_dates = matrix_covid_dates[visible_indices]\n",
    "    # plt.tight_layout()\n",
    "    # plt.xticks(visible_indices, visible_dates, rotation=60)\n",
    "    # # Configure the plot title and axis labels\n",
    "    # plt.autoscale(axis='x', tight=True)\n",
    "    # plt.autoscale(axis='y', tight=True)\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Similarity')\n",
    "    # # Display the legend\n",
    "    # plt.legend()\n",
    "    \n",
    "    # plt.savefig(f'Datas/results/Collection100_{cases}cases.pdf')\n",
    "    # # Display the plot\n",
    "    # plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "    \n",
    "\n",
    "def graph_plot_minimum_casesXMedia():\n",
    "    # # List of plot styles for different measures\n",
    "    # shapes = ['ro-', 'go-', 'yo-', 'mo-', 'co-', 'ko-']\n",
    "\n",
    "    # # Set up the plot figure\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # # Labels for the measures\n",
    "    # labels = ['Degrees', 'Betweenness', 'Clustering', 'Strength', ' Weighted Closeness ', ' Weighted Eigenvector ']\n",
    "    # # Data for each measure\n",
    "    # data = [degrees_avg, betweenness_avg, clustering_avg, strength_avg, closeness_w_avg, eignv_w_avg]\n",
    "    # # Ensure min_cases has the same length as each values array\n",
    "    # min_cases_expanded = [min_cases] * len(data)\n",
    "    # # Iterate over measures, plot the data, and add labels\n",
    "    # for label, values, shape, min_cases_values in zip(labels, data, shapes, min_cases_expanded):\n",
    "    #     plt.plot(min_cases_values, values, shape, label=f'Avg: {round(sum(values) / len(values), 2)} - {label}')\n",
    "    #     # Add text annotations for each point\n",
    "    #     for x, y in zip(min_cases_values, values):\n",
    "    #         plt.text(x, y, str(round(y, 2)), ha='center', va='bottom')\n",
    "    # # Set axis labels\n",
    "    # plt.xlabel('Minimum number of cases')\n",
    "    # plt.ylabel('Average Similarity')\n",
    "    # # Automatically adjust the plot to remove empty space after data\n",
    "    # plt.autoscale(axis='x', tight=True)\n",
    "    # # Add legend\n",
    "    # plt.legend(title='Average Similarity', loc='best')\n",
    "    # # Adjust layout\n",
    "    # plt.tight_layout()\n",
    "    # # Save the plot as an image file\n",
    "    # plt.savefig('Datas/results/graph_minimum-Media.pdf')\n",
    "    # # Close the plot to release resources\n",
    "    # plt.close()\n",
    "\n",
    "    labels = ['Degrees', 'Weighted Betweenness', 'Clustering', 'Strength', 'Weighted Closeness ', 'Weighted Eigenvector ','Upper rand','Lower rand']\n",
    "    data = [degrees_integral, betweenness_integral, clustering_integral, strength_integral, closeness_w_integral, eignv_w_integral,upper_random_avg,lower_random_avg]\n",
    "    with open('table_Integral_Collection_Comparison2.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Minimum number of cases'] + labels)\n",
    "        \n",
    "        for min_case, values in zip(min_cases, zip(*data)):\n",
    "            writer.writerow([min_case] + list(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "      upper_random_avg  lower_random_avg\n",
      "id                                      \n",
      "1             0.000000          0.000000\n",
      "2             0.000000          0.000000\n",
      "3             0.039601         -0.037379\n",
      "4             0.029701         -0.028034\n",
      "5             0.023761         -0.022427\n",
      "...                ...               ...\n",
      "5381          0.999279          0.999236\n",
      "5382          0.999465          0.999422\n",
      "5383          0.999651          0.999608\n",
      "5384          0.999814          0.999814\n",
      "5385          1.000000          1.000000\n",
      "\n",
      "[5385 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for minimum_cases in min_cases:\n",
    "    covidID_list = filter_cases(\"Datas/Pre-processed/cases-brazil-cities-time_2020.csv\", minimum_cases)\n",
    "    metrics_list = filter_records(get_matrix(), covidID_list)\n",
    "    covidID_list = filter_records2(metrics_list,covidID_list)\n",
    "    metrics_matrix = np.array(metrics_list, dtype=float)\n",
    "    covid_matrix = np.array(covidID_list)\n",
    "    # Receive the size of the metrics matrix to match the size of the matrices\n",
    "    matrix_size = len(metrics_matrix)\n",
    "    matrix_covid_dates = covid_matrix[:matrix_size, 0]\n",
    "    Id_matrix_covid = covid_matrix[:matrix_size, 1].astype(float)\n",
    "    result_list = [calculate_random_metric_averages(metrics_matrix) for _ in range(300)]\n",
    "    combined_table = pd.concat(result_list)\n",
    "    mean_table = combined_table.groupby('id').mean()\n",
    "    std_table = combined_table.groupby('id').std()\n",
    "    upper_bound = mean_table + 2 * std_table\n",
    "    lower_bound = mean_table - 2 * std_table\n",
    "    lower_bound_mean = lower_bound.mean(axis=1)\n",
    "    upper_bound_mean = upper_bound.mean(axis=1)\n",
    "    # Calculate the similarity percentage for each metric\n",
    "    degrees_similarity = compare_columns(metrics_matrix, 1)\n",
    "    betweenness_similarity = compare_columns(metrics_matrix, 2)\n",
    "    clustering_similarity = compare_columns(metrics_matrix, 3)\n",
    "    strength_similarity = compare_columns(metrics_matrix, 4)\n",
    "    closeness_w_similarity = compare_columns(metrics_matrix, 5)\n",
    "    eignv_w_similarity = compare_columns(metrics_matrix, 6)\n",
    "    #Calculate the accumulated area under each curve\n",
    "    \n",
    "    matrix_covid_dates2 = pd.to_datetime(matrix_covid_dates)\n",
    "    intervalos_tempo = matrix_covid_dates2.to_series().diff().dt.days\n",
    "    intervalos_tempo = intervalos_tempo.iloc[1:]\n",
    "\n",
    "    degrees_integral.append(np.trapz(degrees_similarity, dx=intervalos_tempo))\n",
    "    betweenness_integral.append(np.trapz(betweenness_similarity, dx=intervalos_tempo))\n",
    "    clustering_integral.append(np.trapz(clustering_similarity, dx=intervalos_tempo))\n",
    "    strength_integral.append(np.trapz(strength_similarity, dx=intervalos_tempo))\n",
    "    closeness_w_integral.append(np.trapz(closeness_w_similarity, dx=intervalos_tempo))\n",
    "    eignv_w_integral.append(np.trapz(eignv_w_similarity, dx=intervalos_tempo))\n",
    "    upper_random_avg.append(np.trapz(upper_bound_mean.values, dx=intervalos_tempo))\n",
    "    lower_random_avg.append(np.trapz(lower_bound_mean.values, dx=intervalos_tempo))\n",
    "    \n",
    "        # Random_mean_integral.append(np.trapz(mean_table,dx=1))\n",
    "        # Random_std_integral.append(np.trapz(std_table,dx=1))\n",
    "       # Calculate average similarity values for each column\n",
    "        # degrees_avg.append(np.mean(degrees_similarity))\n",
    "        # betweenness_avg.append(np.mean(betweenness_similarity))\n",
    "        # clustering_avg.append(np.mean(clustering_similarity))\n",
    "        # strength_avg.append(np.mean(strength_similarity))\n",
    "        # closeness_w_avg.append(np.mean(closeness_w_similarity))\n",
    "        # eignv_w_avg.append(np.mean(eignv_w_similarity))\n",
    "graph_plot_minimum_casesXMedia()\n",
    "\n",
    "#setUp_comparsion_table(min_cases[0])\n",
    "#print(degrees_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
